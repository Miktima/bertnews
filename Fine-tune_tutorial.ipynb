{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6c3975",
   "metadata": {},
   "source": [
    "# Fine-tune ruBERT-tiny2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c2bfd",
   "metadata": {},
   "source": [
    "Загружаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1521b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC \n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90537de",
   "metadata": {},
   "source": [
    "Загружаем отобранные статьи: cтатьи из https://ria.ru/export/rss2/archive/index.xml, которые в течение трех суток были изменены и затем проверены дополнительно через Яндекс Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c463b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datanews.json', encoding=\"utf-8\") as f:\n",
    "    read_data = f.read()\n",
    "read_data = read_data.replace('\\n][\\n', ',\\n')\n",
    "articles = pd.read_json(StringIO(read_data), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d7978",
   "metadata": {},
   "source": [
    "Формируем предложения из отобранных статей. Второе предложение типа \"МОСКВА, 12 янв – РИА Новости\" убираем.\n",
    "В новый DataFrame записываем предложение и признак, что предложения корректные (правильные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06011b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct sentences:  4036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>В Госдуме предложили сделать старый Новый год ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Председатель союза дачников Подмосковья и депу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>\"После длинных новогодних праздников людям тяж...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  В Госдуме предложили сделать старый Новый год ...\n",
       "1       1  Председатель союза дачников Подмосковья и депу...\n",
       "2       1  \"После длинных новогодних праздников людям тяж..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst = nltk.PunktSentenceTokenizer()\n",
    "sentences = pd.DataFrame({\"target\": [], \"text\": []})\n",
    "for ind in articles.index:\n",
    "    sentArticle = pst.tokenize(articles['Article'][ind])\n",
    "    m = 0\n",
    "    for s in sentArticle:\n",
    "        if m != 1:\n",
    "            sentences.loc[len(sentences.index)] = [1, s]\n",
    "print(\"Number of correct sentences: \", len(sentences.index))\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdfa0c",
   "metadata": {},
   "source": [
    "Добавляем предложения с ошибками. При этом устанавливаем признак, что предложения неправильные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c922fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrect sentences:  52\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>0</td>\n",
       "      <td>По шести приграничным районам было выпушено не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>0</td>\n",
       "      <td>Веден временный график подачи воды. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>0</td>\n",
       "      <td>Уточняется, что целью экипажей был опорный пун...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "4085       0  По шести приграничным районам было выпушено не...\n",
       "4086       0             Веден временный график подачи воды. \\n\n",
       "4087       0  Уточняется, что целью экипажей был опорный пун..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list = []\n",
    "with open('errorsents.txt', encoding=\"utf-8\") as fe:\n",
    "    error_list = fe.readlines()\n",
    "for s in error_list:\n",
    "    sentences.loc[len(sentences.index)] = [0, s]\n",
    "print(\"Number of incorrect sentences: \", len(error_list))\n",
    "sentences.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaeef6e",
   "metadata": {},
   "source": [
    "Формируем набор для обучения и тестовый набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17621ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "# train.reset_index()\n",
    "# test.reset_index()\n",
    "train_text = train['text'].astype('str')\n",
    "test_text = test['text'].astype('str')\n",
    "train_labels = train['target']\n",
    "test_labels = test['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce62456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3ad7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.values.tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.values.tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93136651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        # item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = Data(tokens_train, train_labels)\n",
    "test_dataset = Data(tokens_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba9a24",
   "metadata": {},
   "source": [
    "https://www.javatpoint.com/accuracy_score-in-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40141fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9853300733496333\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(Y_true, Y_pred):  \n",
    "    correctly_predicted = 0  \n",
    "    # iterating over every label and checking it with the true sample  \n",
    "    for true_label, predicted in zip(Y_true, Y_pred):  \n",
    "        if true_label == predicted:  \n",
    "            correctly_predicted += 1  \n",
    "    # computing the accuracy score  \n",
    "    accuracy_score = correctly_predicted / len(Y_true)  \n",
    "    return accuracy_score  \n",
    "\n",
    "# Training the model using the Support Vector Classification class of sklearn  \n",
    "svc = SVC()  \n",
    "svc.fit(tokens_train['input_ids'], train_labels)  \n",
    "  \n",
    "# Computing the accuracy score of the model  \n",
    "Y_pred = svc.predict(tokens_train['input_ids'])  \n",
    "score = compute_accuracy(test_labels, Y_pred)  \n",
    "print(score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d9705",
   "metadata": {},
   "source": [
    "Функция для расчета метрики. Используется метрику F1, так как классы не сбалансированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8900c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {'F1': f1}\n",
    "# import evaluate\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# def compute_metrics(pred):\n",
    "    # logits, labels = pred\n",
    "    # predictions = np.argmax(logits, axis=-1)\n",
    "    # return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68477b0c",
   "metadata": {},
   "source": [
    "Ниже указаны все параметры, которые будут использоваться для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0842a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './results', #Выходной каталог\n",
    "    # num_train_epochs = 3, #Кол-во эпох для обучения\n",
    "    num_train_epochs = 16, #Кол-во эпох для обучения\n",
    "    # per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n",
    "    # per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n",
    "    # weight_decay =0.01, #Понижение весов\n",
    "    weight_decay =0.1, #Понижение весов\n",
    "    logging_dir = './logs', #Каталог для хранения журналов\n",
    "    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n",
    "    learning_rate = 1e-5, #Скорость обучения\n",
    "    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
    "    logging_strategy = 'epoch', #Логирование после каждой эпохи\n",
    "    save_strategy = 'epoch', #Сохранение после каждой эпохи\n",
    "    save_total_limit = 1,\n",
    "    seed=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91bb3e",
   "metadata": {},
   "source": [
    "Передача в trainer предообученную модель, tokenizer, данные для обучения, данные для валидации и способ расчета метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "697d9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  tokenizer = tokenizer,\n",
    "                  args = training_args,\n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset = test_dataset,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6850894",
   "metadata": {},
   "source": [
    "Запуск обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980715ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c4b1545254e229b2eb350c34a45ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10504/16966540.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2776\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2778\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2779\u001b[0m         )\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c86e6f",
   "metadata": {},
   "source": [
    "Сохранение обученной модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540f2b6",
   "metadata": {},
   "source": [
    "model_path = \"fine-tune-rubert-tiny2\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aed234",
   "metadata": {},
   "source": [
    "Написание функции для получения предикта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9cccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction():\n",
    "    test_pred = trainer.predict(test_dataset)\n",
    "    labels = np.argmax(test_pred.predictions, axis = -1)\n",
    "    return labels\n",
    "pred = get_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05efc94",
   "metadata": {},
   "source": [
    "Вывод всей необходимой информации для оценки качества модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd43739",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, pred))\n",
    "print(f1_score(test_labels, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertnews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
