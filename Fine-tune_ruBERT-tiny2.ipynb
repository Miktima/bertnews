{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6c3975",
   "metadata": {},
   "source": [
    "# Fine-tune ruBERT-tiny2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c2bfd",
   "metadata": {},
   "source": [
    "Загружаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1521b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from transformers import AutoModel, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC \n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90537de",
   "metadata": {},
   "source": [
    "Загружаем отобранные статьи. Статьи из https://ria.ru/export/rss2/archive/index.xml, которые в течение трех суток были изменены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c463b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datanews.json', encoding=\"utf-8\") as f:\n",
    "    read_data = f.read()\n",
    "read_data = read_data.replace('\\n][\\n', ',\\n')\n",
    "articles = pd.read_json(StringIO(read_data), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d7978",
   "metadata": {},
   "source": [
    "Формируем предложения из проверенных дважы статей. Второе предложение типа \"МОСКВА, 12 янв – РИА Новости\" убираем.\n",
    "В новый DataFrame записываем предложение и признак, что предложения корректые (правильные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06011b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct sentences:  927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В Госдуме предложили сделать старый Новый год ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Председатель союза дачников Подмосковья и депу...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"После длинных новогодних праздников людям тяж...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  В Госдуме предложили сделать старый Новый год ...      1\n",
       "1  Председатель союза дачников Подмосковья и депу...      1\n",
       "2  \"После длинных новогодних праздников людям тяж...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst = nltk.PunktSentenceTokenizer()\n",
    "sentences = pd.DataFrame({\"sentence\": [], \"label\": []})\n",
    "for ind in articles.index:\n",
    "    sentArticle = pst.tokenize(articles['Article'][ind])\n",
    "    m = 0\n",
    "    for s in sentArticle:\n",
    "        if m != 1:\n",
    "            sentences.loc[len(sentences.index)] = [s, 1]\n",
    "print(\"Number of correct sentences: \", len(sentences.index))\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdfa0c",
   "metadata": {},
   "source": [
    "Добавляем предложения с ошибками. При этом устанавливаем признак, что предложения неправильные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c922fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrect sentences:  5\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "with open('errorsents.txt', encoding=\"utf-8\") as fe:\n",
    "    error_list = fe.readlines()\n",
    "for s in error_list:\n",
    "    sentences.loc[len(sentences.index)] = [s, 0]\n",
    "print(\"Number of incorrect sentences: \", len(error_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaeef6e",
   "metadata": {},
   "source": [
    "Формируем набор для обучения и тестовый набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17621ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "train_text = train['sentence'].astype('str')\n",
    "test_text = test['sentence'].astype('str')\n",
    "train_labels = train['label']\n",
    "test_labels = test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2dc5d",
   "metadata": {},
   "source": [
    "Задание всех seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d39490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce62456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ad7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.values.tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.values.tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93136651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = Data(tokens_train, train_labels)\n",
    "test_dataset = Data(tokens_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba9a24",
   "metadata": {},
   "source": [
    "https://www.javatpoint.com/accuracy_score-in-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40141fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9893048128342246\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(Y_true, Y_pred):  \n",
    "    correctly_predicted = 0  \n",
    "    # iterating over every label and checking it with the true sample  \n",
    "    for true_label, predicted in zip(Y_true, Y_pred):  \n",
    "        if true_label == predicted:  \n",
    "            correctly_predicted += 1  \n",
    "    # computing the accuracy score  \n",
    "    accuracy_score = correctly_predicted / len(Y_true)  \n",
    "    return accuracy_score  \n",
    "\n",
    "# Training the model using the Support Vector Classification class of sklearn  \n",
    "svc = SVC()  \n",
    "svc.fit(tokens_train['input_ids'], train_labels)  \n",
    "  \n",
    "# Computing the accuracy score of the model  \n",
    "Y_pred = svc.predict(tokens_train['input_ids'])  \n",
    "score = compute_accuracy(test_labels, Y_pred)  \n",
    "print(score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d9705",
   "metadata": {},
   "source": [
    "Функция для расчета метрики. Используется метрику F1, так как классы не сбалансированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8900c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {'F1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68477b0c",
   "metadata": {},
   "source": [
    "Ниже указаны все параметры, которые будут использоваться для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0842a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './results', #Выходной каталог\n",
    "    num_train_epochs = 3, #Кол-во эпох для обучения\n",
    "    per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n",
    "    per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n",
    "    weight_decay =0.01, #Понижение весов\n",
    "    logging_dir = './logs', #Каталог для хранения журналов\n",
    "    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n",
    "    learning_rate = 1e-5, #Скорость обучения\n",
    "    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
    "    logging_strategy = 'epoch', #Логирование после каждой эпохи\n",
    "    save_strategy = 'epoch', #Сохранение после каждой эпохи\n",
    "    save_total_limit = 1,\n",
    "    seed=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91bb3e",
   "metadata": {},
   "source": [
    "Передача в trainer предообученную модель, tokenizer, данные для обучения, данные для валидации и способ расчета метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697d9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  tokenizer = tokenizer,\n",
    "                  args = training_args,\n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset = train_dataset,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6850894",
   "metadata": {},
   "source": [
    "Запуск обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980715ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AcceleratorState' object has no attribute 'distributed_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1551\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1553\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;66;03m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;66;03m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m total_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mworld_size\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/transformers/trainer.py:804\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[1;32m    802\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n\u001b[0;32m--> 804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/accelerate/accelerator.py:1183\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1174\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1177\u001b[0m     ):\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1180\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1181\u001b[0m         )\n\u001b[0;32m-> 1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1184\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/dev/bertnews/lib/python3.10/site-packages/accelerate/accelerator.py:495\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AcceleratorState' object has no attribute 'distributed_type'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c86e6f",
   "metadata": {},
   "source": [
    "Сохранение обученной модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540f2b6",
   "metadata": {},
   "source": [
    "model_path = \"fine-tune-rubert-tiny2\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aed234",
   "metadata": {},
   "source": [
    "Написание функции для получения предикта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9cccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction():\n",
    "    test_pred = trainer.predict(test_dataset)\n",
    "    labels = np.argmax(test_pred.predictions, axis = -1)\n",
    "    return labels\n",
    "pred = get_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05efc94",
   "metadata": {},
   "source": [
    "Вывод всей необходимой информации для оценки качества модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd43739",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, pred))\n",
    "print(f1_score(test_labels, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertnews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
