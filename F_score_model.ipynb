{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26260,"status":"ok","timestamp":1710605240575,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"},"user_tz":-180},"id":"xzDVvbey-GNH","outputId":"a4b2f077-be2a-4560-d6c8-4d12cbcc7d8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}],"source":["!pip install transformers[sentencepiece]\n","!pip install --user -U nltk"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UWxsqgKJ-ZFi","executionInfo":{"status":"ok","timestamp":1710605256296,"user_tz":-180,"elapsed":15730,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["import re\n","import requests\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, pipeline, logging\n","import nltk\n","import time\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"2zs4c_sNXS9C"},"source":["Подсоединение к Google Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rpQSeXZZ-luR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710605279993,"user_tz":-180,"elapsed":23725,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}},"outputId":"b6ba2f16-a97d-4906-df44-81b6c477d9a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CokBGGMH5UKE","executionInfo":{"status":"ok","timestamp":1710605289363,"user_tz":-180,"elapsed":9380,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}},"outputId":"ef7776f5-46f7-4deb-9e48-167357311e16"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Time initialization:  9.546213150024414\n"]}],"source":["initTime = time.time()\n","\n","nltk.download('punkt')\n","\n","model_checkpoint = \"/content/drive/My Drive/fine-train-rubertbase20240307\"\n","# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruRoberta-large\")\n","model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n","# tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","print (\"Time initialization: \", time.time() - initTime)\n","logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cIPbgDQxkAvO","executionInfo":{"status":"ok","timestamp":1710605289715,"user_tz":-180,"elapsed":378,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["with open('/content/drive/My Drive/errorsents.txt', encoding=\"utf-8\") as f:\n","  sentmarked_list = f.readlines()"]},{"cell_type":"markdown","metadata":{"id":"Ug4zSyUKYCbe"},"source":["Собираем информацию о словах с ошибками"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pxNVEYfg3QTY","executionInfo":{"status":"ok","timestamp":1710605289716,"user_tz":-180,"elapsed":12,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["errPattern = re.compile(\"_([\\w\\s-]+)_\")\n","sent_list = []\n","errtup_list = []\n","for s in sentmarked_list:\n","    errIter = errPattern.finditer(s)\n","    ss = re.sub(\"_\", \"\", s)\n","    sent_list.append(ss)\n","    j = 0\n","    errtup = []\n","    for err in errIter:\n","        errtup.append((err.start()-j*2, err.end()-2*(1 + j)))\n","        j += 1\n","    errtup_list.append(errtup)"]},{"cell_type":"markdown","source":["Загружаем предложения без ошибок.\n","В последствии здесь тоже могут выявлены ошибки, поэтому также\n","формируем errors_pos"],"metadata":{"id":"bx5LcWIyzjmq"}},{"cell_type":"code","source":["with open('/content/drive/My Drive/correctsents.txt', encoding=\"utf-8\") as f:\n","  sentcorrect_list = f.readlines()"],"metadata":{"id":"_-GswMIrzuDS","executionInfo":{"status":"ok","timestamp":1710605290010,"user_tz":-180,"elapsed":300,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for s in sentcorrect_list:\n","    errIter = errPattern.finditer(s)\n","    ss = re.sub(\"_\", \"\", s)\n","    sent_list.append(ss)\n","    j = 0\n","    errtup = []\n","    for err in errIter:\n","        errtup.append((err.start()-j*2, err.end()-2*(1 + j)))\n","        j += 1\n","    errtup_list.append(errtup)"],"metadata":{"id":"3zhm9OGyz4IN","executionInfo":{"status":"ok","timestamp":1710605290011,"user_tz":-180,"elapsed":9,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HC7B3JMR45Xn"},"source":["Собираем DataFrame"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"F2aJpiNk49VT","executionInfo":{"status":"ok","timestamp":1710605290271,"user_tz":-180,"elapsed":268,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["sentences = pd.DataFrame(list(zip(sent_list, errtup_list)), columns =['sentences', 'errors_pos'])"]},{"cell_type":"markdown","metadata":{"id":"rtziCMM35BXu"},"source":["Начальные параметры"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"yO3YJUKD8qfM","executionInfo":{"status":"ok","timestamp":1710605290272,"user_tz":-180,"elapsed":16,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["p = re.compile(r'[\\w-]+')\n","\n","TP = 0\n","TN = 0\n","FP = 0\n","FN = 0"]},{"cell_type":"markdown","metadata":{"id":"TtO8iv0H8yRq"},"source":["Маскирование токенов в предложении и определение ошибок"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"q4EKac2G84Vk","executionInfo":{"status":"ok","timestamp":1710605290272,"user_tz":-180,"elapsed":14,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["def mask_bert_sent_old(text, model, tokenizer):\n","    maskToken = \"[MASK]\"\n","    # masking whole text and return errors if available\n","    tr = 5e-4 #threshold of the error (parameter !!!)\n","    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n","    p = re.compile(r'[\\w-]+')\n","    sentArticle = nltk.tokenize.sent_tokenize(text, language=\"russian\")\n","    listErr = []\n","    ind = 0\n","    for s in sentArticle:\n","        ind = text.index(s)\n","        iter = p.finditer(s)\n","        for match in iter:\n","            if match.start() == 0:\n","                masktext = maskToken + s[match.end():]\n","            elif match.end() == len(s):\n","                masktext = s[:match.start()] + maskToken\n","            else:\n","                masktext = s[:match.start()] + maskToken + s[match.end():]\n","            res = unmasker(masktext, targets=[match.group()], batch_size=8)\n","            if res[0]['score'] < tr:\n","                errorrDesc = {\n","                    \"word\": match.group(),\n","                    \"start\": ind + match.start(),\n","                    \"end\": ind + match.end(),\n","                    \"prob\": res[0]['score']\n","                }\n","                listErr.append(errorrDesc)\n","    return listErr"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TsDXNGxzRSEb","executionInfo":{"status":"ok","timestamp":1710605290272,"user_tz":-180,"elapsed":12,"user":{"displayName":"Иван Иванов","userId":"15901592262296195513"}}},"outputs":[],"source":["def mask_bert_sent(text, model, tokenizer):\n","    # masking whole text and return errors if available\n","    tr = 5e-5 #threshold of the error (parameter !!!)\n","    sentArticle = nltk.tokenize.sent_tokenize(text, language=\"russian\")\n","\n","    p = re.compile(r'[\\w-]+')\n","    listErr = []\n","    ind = 0\n","\n","    for s in sentArticle:\n","      inputs = tokenizer(s, return_tensors=\"pt\")\n","      # print (inputs.word_ids)\n","      list_errtoken = []\n","      for i, x in enumerate(inputs[\"input_ids\"][0]):\n","        if tokenizer.decode(x) not in tokenizer.all_special_tokens:\n","          masked_token = x.item()\n","          inputs[\"input_ids\"][0][i] = tokenizer.mask_token_id\n","          token_logits = model(**inputs).logits\n","\n","          # Find the location of [MASK] and extract its logits\n","          mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n","          mask_token_logits = token_logits[0, mask_token_index, :]\n","\n","          probs = torch.nn.functional.softmax(mask_token_logits, dim=1)\n","          spanchar = inputs.token_to_chars(i)\n","          # Add to error tokens, if threshold > probability\n","          if tr > probs[0][masked_token].item():\n","            list_errtoken.append([spanchar.start, probs[0][masked_token].item()])\n","          # print (f'Probability of masked token ({tokenizer.decode(masked_token)}): {probs[0][masked_token].item()},  spanchar: {spanchar} ')\n","          inputs[\"input_ids\"][0][i] = masked_token\n","\n","      iter = p.finditer(s)\n","      istored = -1\n","      for idx, word in enumerate(iter):\n","        for terr in list_errtoken:\n","          # if start char of found token into the selected word\n","          # store this word once\n","          if terr[0] >= word.start() and terr[0] < word.end():\n","            if istored != idx:\n","              errorrDesc = {\n","                \"word\": word.group(),\n","                \"start\": ind + word.start(),\n","                \"end\": ind + word.end(),\n","                \"prob\": terr[1]\n","              }\n","              listErr.append(errorrDesc)\n","            istored = idx\n","\n","    #print (\"------------------\")\n","    #print(*[(token_id, inputs.token_to_chars(idx)) for idx, token_id in enumerate(inputs.input_ids[0])], sep=\"\\n\")\n","    #print (\"------------------\")\n","    #print(*[(word, inputs.word_to_tokens(idx)) for idx, word in enumerate(text.split())], sep=\"\\n\")\n","    return listErr"]},{"cell_type":"markdown","metadata":{"id":"vH1efce49RRM"},"source":["Перебор всех предложений с DataFrame и сбор сведений для расчета метрики"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpzXTNGA9bZi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5aa1ef06-2ba8-4a5a-aa91-6760962c7ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Index:  0    Time: 10.445069313049316\n","Index:  10    Time: 57.21166014671326\n"]}],"source":["outerrors = []\n","startPos = 0\n","for ind in sentences.index:\n","    if (ind % 10) == 0:\n","        print (\"Index: \", ind, \"   Time:\", time.time() - initTime)\n","    response = mask_bert_sent(sentences[\"sentences\"][ind], model, tokenizer)\n","    if len(response) == 0:\n","        # No errors found\n","        ntoken = len(p.findall(sentences[\"sentences\"][ind]))\n","        nerr = len(sentences[\"errors_pos\"][ind])\n","        # True Negative = number of words without errors\n","        TN += ntoken - nerr\n","        # False Negative - nerr errors were not found\n","        FN += nerr\n","        article_err = sentences[\"sentences\"][ind]\n","    else:\n","        # errors were found: iter by found errors\n","        nfound = 0\n","        startPos = 0\n","        article_err = \"\"\n","        for e in response:\n","            article_err += sentences[\"sentences\"][ind][startPos:e['start']] + \"<mark>\" + sentences[\"sentences\"][ind][e['start']:e['end']] + \"</mark>\"\n","            startPos = e['end']\n","            # found errors correct or not\n","            iscorrect = 0\n","            for trueerr in sentences[\"errors_pos\"][ind]:\n","                if trueerr[0] == e[\"start\"]:\n","                    iscorrect = 1\n","                    nfound += 1\n","                    break\n","            if iscorrect == 1:\n","                # True Positive - error word was found\n","                TP += 1\n","        article_err += sentences[\"sentences\"][ind][startPos:]\n","        # False Positive - wrong errors were found\n","        if (len(response) - nfound) > 0:\n","            FP += len(response) - nfound\n","        # False Negative - errors were not found\n","        if (len(sentences[\"errors_pos\"][ind]) - nfound) > 0:\n","            FN += len(sentences[\"errors_pos\"][ind]) - nfound\n","    outerrors.append(article_err)"]},{"cell_type":"markdown","metadata":{"id":"SnixusQO0dp0"},"source":["Сохранение файла с метками"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4YwsYbJ0pF0"},"outputs":[],"source":["with open('/content/drive/My Drive/errorsmarks.txt', \"w\", encoding=\"utf-8\") as file:\n","  file.writelines(outerrors)"]},{"cell_type":"markdown","metadata":{"id":"ewsX9Oys9fCa"},"source":["Расчет метрики"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dRRTZ2Z9iIb"},"outputs":[],"source":["precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","fmeasure = 2 * recall * precision / (recall + precision)\n","print (\"Precision: \", precision)\n","print (\"Recall: \", recall)\n","print (\"F-measure: \", fmeasure)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN/nWrxhtw0p1vi31L4isir"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}